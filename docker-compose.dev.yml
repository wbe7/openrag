# Development Docker Compose - builds Langflow from source
# Usage: GIT_BRANCH=test-openai-responses docker compose -f docker-compose.dev.yml up -d
# Or use Make commands: make dev-branch BRANCH=test-openai-responses
#
# Dev ports (different from production to allow parallel running):
#   Frontend:   3001 (prod: 3000)
#   Langflow:   7861 (prod: 7860)
#   OpenSearch: 9201 (prod: 9200)
#   Dashboards: 5602 (prod: 5601)

services:
  opensearch:
    image: langflowai/openrag-opensearch:${OPENRAG_VERSION:-latest}
    container_name: os-dev
    depends_on:
      - openrag-backend
    environment:
      - discovery.type=single-node
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_PASSWORD}
    command: >
      bash -c "
        # Ensure data directory has correct permissions
        sudo chown -R opensearch:opensearch /usr/share/opensearch/data || true
        
        # Start OpenSearch in background
        /usr/share/opensearch/opensearch-docker-entrypoint.sh opensearch &

        # Wait a bit for OpenSearch to start, then apply security config
        sleep 10 && /usr/share/opensearch/setup-security.sh &

        # Wait for background processes
        wait
      "
    ports:
      - "9201:9200"
      - "9601:9600"
    volumes:
      - ${OPENSEARCH_DATA_PATH:-./opensearch-data-dev}:/usr/share/opensearch/data:U,z

  dashboards:
    image: opensearchproject/opensearch-dashboards:3.0.0
    container_name: osdash-dev
    depends_on:
      - opensearch
    environment:
      OPENSEARCH_HOSTS: '["https://opensearch:9200"]'
      OPENSEARCH_USERNAME: "admin"
      OPENSEARCH_PASSWORD: ${OPENSEARCH_PASSWORD}
    ports:
      - "5602:5601"

  openrag-backend:
    image: langflowai/openrag-backend:${OPENRAG_VERSION:-latest}
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: openrag-backend-dev
    depends_on:
      - langflow
    environment:
      - OPENSEARCH_HOST=opensearch
      - LANGFLOW_URL=http://langflow:7860
      - LANGFLOW_PUBLIC_URL=${LANGFLOW_PUBLIC_URL}
      - LANGFLOW_AUTO_LOGIN=${LANGFLOW_AUTO_LOGIN}
      - LANGFLOW_SUPERUSER=${LANGFLOW_SUPERUSER}
      - LANGFLOW_SUPERUSER_PASSWORD=${LANGFLOW_SUPERUSER_PASSWORD}
      - LANGFLOW_CHAT_FLOW_ID=${LANGFLOW_CHAT_FLOW_ID}
      - LANGFLOW_INGEST_FLOW_ID=${LANGFLOW_INGEST_FLOW_ID}
      - LANGFLOW_URL_INGEST_FLOW_ID=${LANGFLOW_URL_INGEST_FLOW_ID}
      - DISABLE_INGEST_WITH_LANGFLOW=${DISABLE_INGEST_WITH_LANGFLOW:-false}
      - NUDGES_FLOW_ID=${NUDGES_FLOW_ID}
      - OPENSEARCH_PORT=9200
      - OPENSEARCH_USERNAME=admin
      - OPENSEARCH_PASSWORD=${OPENSEARCH_PASSWORD}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - WATSONX_API_KEY=${WATSONX_API_KEY}
      - WATSONX_ENDPOINT=${WATSONX_ENDPOINT}
      - WATSONX_PROJECT_ID=${WATSONX_PROJECT_ID}
      - OLLAMA_ENDPOINT=${OLLAMA_ENDPOINT}
      - GOOGLE_OAUTH_CLIENT_ID=${GOOGLE_OAUTH_CLIENT_ID}
      - GOOGLE_OAUTH_CLIENT_SECRET=${GOOGLE_OAUTH_CLIENT_SECRET}
      - MICROSOFT_GRAPH_OAUTH_CLIENT_ID=${MICROSOFT_GRAPH_OAUTH_CLIENT_ID}
      - MICROSOFT_GRAPH_OAUTH_CLIENT_SECRET=${MICROSOFT_GRAPH_OAUTH_CLIENT_SECRET}
      - WEBHOOK_BASE_URL=${WEBHOOK_BASE_URL}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ${OPENRAG_DOCUMENTS_PATH:-./openrag-documents}:/app/openrag-documents:Z
      - ${OPENRAG_KEYS_PATH:-./keys}:/app/keys:U,z
      - ${OPENRAG_FLOWS_PATH:-./flows}:/app/flows:U,z
      - ${OPENRAG_CONFIG_PATH:-./config}:/app/config:Z
      - ${OPENRAG_DATA_PATH:-./data}:/app/data:Z

  openrag-frontend:
    image: langflowai/openrag-frontend:${OPENRAG_VERSION:-latest}
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: openrag-frontend-dev
    depends_on:
      - openrag-backend
    environment:
      - OPENRAG_BACKEND_HOST=openrag-backend
    ports:
      - "3001:3000"

  langflow:
    build:
      context: .
      dockerfile: Dockerfile.langflow.dev
      args:
        GIT_REPO: ${GIT_REPO:-https://github.com/langflow-ai/langflow.git}
        GIT_BRANCH: ${GIT_BRANCH:-main}
      # Increase memory for build (npm build is memory intensive)
      shm_size: '2gb'
    container_name: langflow-dev
    ports:
      - "7861:7860"
    volumes:
      - ${OPENRAG_FLOWS_PATH:-./flows}:/app/flows:U,z
    environment:
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY:-}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY:-}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-None}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-None}
      - WATSONX_API_KEY=${WATSONX_API_KEY:-None}
      - WATSONX_ENDPOINT=${WATSONX_ENDPOINT:-None}
      - WATSONX_PROJECT_ID=${WATSONX_PROJECT_ID:-None}
      - OLLAMA_BASE_URL=${OLLAMA_ENDPOINT:-None}
      - LANGFLOW_LOAD_FLOWS_PATH=/app/flows
      - LANGFLOW_SECRET_KEY=${LANGFLOW_SECRET_KEY}
      - JWT=None  
      - OWNER=None
      - OWNER_NAME=None
      - OWNER_EMAIL=None
      - CONNECTOR_TYPE=system
      - CONNECTOR_TYPE_URL=url
      - OPENRAG-QUERY-FILTER="{}"
      - OPENSEARCH_PASSWORD=${OPENSEARCH_PASSWORD}
      - FILENAME=None
      - MIMETYPE=None
      - FILESIZE=0
      - SELECTED_EMBEDDING_MODEL=${SELECTED_EMBEDDING_MODEL:-}
      - LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT=JWT,OPENRAG-QUERY-FILTER,OPENSEARCH_PASSWORD,OWNER,OWNER_NAME,OWNER_EMAIL,CONNECTOR_TYPE,FILENAME,MIMETYPE,FILESIZE,SELECTED_EMBEDDING_MODEL,OPENAI_API_KEY,ANTHROPIC_API_KEY,WATSONX_API_KEY,WATSONX_ENDPOINT,WATSONX_PROJECT_ID,OLLAMA_BASE_URL
      - LANGFLOW_LOG_LEVEL=DEBUG
      - LANGFLOW_AUTO_LOGIN=${LANGFLOW_AUTO_LOGIN}
      - LANGFLOW_SUPERUSER=${LANGFLOW_SUPERUSER}
      - LANGFLOW_SUPERUSER_PASSWORD=${LANGFLOW_SUPERUSER_PASSWORD}
      - LANGFLOW_NEW_USER_IS_ACTIVE=${LANGFLOW_NEW_USER_IS_ACTIVE}
      - LANGFLOW_ENABLE_SUPERUSER_CLI=${LANGFLOW_ENABLE_SUPERUSER_CLI}
      - HIDE_GETTING_STARTED_PROGRESS=true
